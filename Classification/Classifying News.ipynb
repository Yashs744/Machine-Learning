{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifying News\n",
    "\n",
    "I've downloaded news from InShorts website using a Python Script.\n",
    "News are sorted into 5 categories Automobile, Enterainment, Hatke, Science and Technology.\n",
    "Each category contains atleast 1000 News for training the model and 25 each for testing the model\n",
    "\n",
    "###### Author: Yash Sharma\n",
    "###### Date: 10th July 2017\n",
    "###### Library Used: sklearn \n",
    "\n",
    "##### sklearn official [documentation](http://scikit-learn.org/stable/documentation.html) used as a reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing methods from sklearn library\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier, RidgeClassifierCV, RidgeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading training data from InShorts_News/Train dir\n",
    "category = ['Automobile', 'Entertainment', 'Hatke', 'Science', 'Technology']\n",
    "data_train = load_files('InShorts_News/Train', random_state=42, shuffle=True, categories = category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Name: ['Automobile', 'Entertainment', 'Hatke', 'Science', 'Technology']\n"
     ]
    }
   ],
   "source": [
    "# Target Names\n",
    "print (\"Target Name: {}\".format(data_train.target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting features from text files\n",
    "\n",
    "## Bag of Words: \n",
    "The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears.\n",
    "We'll use CountVectorizer function for sklearn librrary to create a bag of words.\n",
    "\n",
    "Initialize the \"CountVectorizer\" object, which is scikit-learn's bag of words tool.  \n",
    "> vectorizer = CountVectorizer()\n",
    "\n",
    "You can also pass various arguments like whether to remove stopwords or tokenize text or maximum number of features we wan.\n",
    "[Here](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) is the complete list of argument we can pass\n",
    "\n",
    "Then we call the fit_transform method and pass the data we want to vectorize\n",
    "> fit_transform(data.data)\n",
    "\n",
    "fit_transform() does two functions: \n",
    "1. It fits the model and learns the vocabulary.\n",
    "2. It transforms our training data into feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english', ngram_range=(1, 1))\n",
    "\n",
    "# Fitting our data\n",
    "X_train_count = vectorizer.fit_transform(data_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after vectorizing  (7770, 27186)\n"
     ]
    }
   ],
   "source": [
    "# Shape of the data_features\n",
    "print (\"\\nShape after vectorizing \", X_train_count.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Occurrences to Frequencies\n",
    "\n",
    "Longer documents have higher average count values than shorter documents, even though they might talk about the same topics and this causes issue. So, to avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document. These new features are called the Term Frequencies.\n",
    "\n",
    "Luckly sklearn learn has a method for this, TfidfTransformer\n",
    "Tf–idf stands for “Term Frequency times Inverse Document Frequency”.\n",
    "\n",
    "What it does is, it downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus. This is also a refinimentt to the make our model fit better.\n",
    "\n",
    "> tfidf = TfidfTransformer()\n",
    "\n",
    "> tfidf_features = tfidf.fit_transform(data_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True)\n",
    "\n",
    "# fit the transformer\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Shape after applying Tf-idf (7770, 27186)\n"
     ]
    }
   ],
   "source": [
    "# Shape of tfidf_features\n",
    "print (\"\\nShape after applying Tf-idf\", X_train_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: \n",
    "\n",
    "The two steps we performed can be merged into a single step using the TfidfVectorizer() method. It is a mixture of CountVectorizer() and TfidfTransformer() method. It first vectorize the data and then form tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Data\n",
    "Load the test data into the memory and perform the same process (Bag of Word and Frequence)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loading test data passing the same categories as of training data\n",
    "data_test = load_files('InShorts_News/Test', random_state=42, shuffle=True, categories = category)\n",
    "\n",
    "# storing text and target in two variables\n",
    "X_test = data_test.data\n",
    "y_test = data_test.target\n",
    "\n",
    "# Performing vectorization and tf-idf on test data\n",
    "X_test_count = vectorizer.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train our model\n",
    "\n",
    "## 1. MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clf stands for classifier\n",
    "clf = MultinomialNB()\n",
    "\n",
    "# fit (train) our model\n",
    "clf.fit(X_train_tfidf, data_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first test our model on some sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entertainment\n",
      "Science\n"
     ]
    }
   ],
   "source": [
    "# First text is for Entertainment and second for Science\n",
    "sample_text = [\n",
    "    \"A wax statue of actor Ranveer Singh, who turned 32 today, was unveiled at the Grévin wax museum in Paris. Ranveer is the third Bollywood celebrity after Shah Rukh Khan and Aishwarya Rai to get a statue at the museum. \\\"It's a truly special birthday present and one that will always bring back beautiful memories of Paris,\\\" said Ranveer.\",\n",
    "    \n",
    "    \"Some physicists suggest that if quantum world is real and time-symmetric, it must allow for 'retrocausality', implying influences can travel backwards in time.\\\n",
    "     Retrocausality, however, doesn't imply that signals can be communicated from future to the past, scientists added. \\\n",
    "     If found correct, it would prove Einstein correct that quantum theory is incomplete and quantum entanglement is not possible.\"\n",
    "]\n",
    "\n",
    "# Vectorize sample data\n",
    "X_new_counts = vectorizer.transform(sample_text)\n",
    "X_new_tfidf = tfidf.transform(X_new_counts)\n",
    "\n",
    "# predict the output\n",
    "predicted = clf.predict(X_new_tfidf)\n",
    "\n",
    "# Print the prediction made by the model\n",
    "for pred in predicted:\n",
    "    print (data_train.target_names[pred])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It worked perfectly. Our Model guessed both sample text perfectly correct.\n",
    "\n",
    "Let's test our model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Making predictions on test data\n",
    "y_mnb_predict = clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Accuracy Score for MultinomialNB Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Automobile       0.80      0.64      0.71        25\n",
      "Entertainment       0.86      1.00      0.93        25\n",
      "        Hatke       1.00      0.40      0.57        25\n",
      "      Science       0.68      1.00      0.81        25\n",
      "   Technology       0.69      0.80      0.74        25\n",
      "\n",
      "  avg / total       0.81      0.77      0.75       125\n",
      "\n",
      "Accuracy Score: 0.768\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(classification_report(y_test, y_mnb_predict, target_names=data_test.target_names))\n",
    "\n",
    "# accuracy score\n",
    "print (\"Accuracy Score: {}\".format(accuracy_score(y_test, y_mnb_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.001, average=False, class_weight=None, epsilon=0.1,\n",
       "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
       "       learning_rate='optimal', loss='hinge', n_iter=5, n_jobs=1,\n",
       "       penalty='l2', power_t=0.5, random_state=42, shuffle=True, verbose=0,\n",
       "       warm_start=False)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM classifier\n",
    "svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.001, n_iter=5, random_state=42)\n",
    "\n",
    "# fit out training data\n",
    "svm_clf.fit(X_train_tfidf, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction using svm model\n",
    "y_svm_predict = svm_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Accuracy Score for SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Automobile       0.69      0.80      0.74        25\n",
      "Entertainment       0.88      0.92      0.90        25\n",
      "        Hatke       1.00      0.44      0.61        25\n",
      "      Science       0.69      1.00      0.82        25\n",
      "   Technology       0.83      0.76      0.79        25\n",
      "\n",
      "  avg / total       0.82      0.78      0.77       125\n",
      "\n",
      "\n",
      "Accuracy Score: 0.784\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(classification_report(y_test, y_svm_predict, target_names=data_test.target_names))\n",
    "\n",
    "# accuracy score\n",
    "print (\"\\nAccuracy Score: {}\".format(accuracy_score(y_test, y_svm_predict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RidgeClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RidgeClassifierCV(alphas=(0.01, 0.1, 1.0), class_weight=None, cv=None,\n",
       "         fit_intercept=True, normalize=False, scoring=None)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ridge classifier\n",
    "ridge_clf = RidgeClassifierCV(alphas=(0.01, 0.1, 1.0))\n",
    "\n",
    "# fit out training data\n",
    "ridge_clf.fit(X_train_tfidf, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make prediction using svm model\n",
    "y_ridge_predict = ridge_clf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix and Accuracy Score for SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Automobile       0.88      0.56      0.68        25\n",
      "Entertainment       0.89      1.00      0.94        25\n",
      "        Hatke       1.00      0.64      0.78        25\n",
      "      Science       0.78      1.00      0.88        25\n",
      "   Technology       0.64      0.84      0.72        25\n",
      "\n",
      "  avg / total       0.84      0.81      0.80       125\n",
      "\n",
      "\n",
      "Accuracy Score: 0.808\n",
      "\n",
      "Alpha: 1.0\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix\n",
    "print(classification_report(y_test, y_ridge_predict, target_names=data_test.target_names))\n",
    "\n",
    "# accuracy score\n",
    "print (\"\\nAccuracy Score: {}\".format(accuracy_score(y_test, y_ridge_predict)))\n",
    "\n",
    "# Best Alpha out of other.\n",
    "# RidgeClassifierCV does the cross-validation it self. We just need to pass a list of alphas and it takes the best one\n",
    "# from the list.\n",
    "print (\"\\nAlpha:\", ridge_clf.alpha_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Building Pipeline\n",
    "All the steps we performed so far we can reduce them by building an pipeline.\n",
    "\n",
    "Pipeline are used to chain multiple estimators (we can add classifiers also) into one, this is useful as many times there are fixed number of steps for a process. To make our work easy we create a pipeline which handles the process for us step by step.\n",
    "    \n",
    "Pipeline serves two purposes here:\n",
    "\n",
    "##### Convenience: \n",
    "    You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "##### Joint parameter selection: \n",
    "    You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "#### Note: All estimators in a pipeline, except the last one, must be transformers (i.e. must have a transform method). The last estimator may be any type (transformer, classifier, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline with CountVectorize, TfidfTransformer and SVM\n",
    "\n",
    "# Now we don't need to call CountVecotrizer and then TfidfTransformer\n",
    "# Pipeline will do this for us.\n",
    "# Pipeline will perform the processing step by step\n",
    "\n",
    "pipe = Pipeline ([\n",
    "    ('countvectorizer', CountVectorizer(stop_words='english', ngram_range=(1, 1))),\n",
    "    ('tfidf', TfidfTransformer(use_idf=True)),\n",
    "    ('ridge', RidgeClassifier(alpha=1))\n",
    "    #('BNB', BernoulliNB(alpha=1))\n",
    "    #('MNB', MultinomialNB(alpha=0.1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit the dataset and make prediction\n",
    "pipe.fit(data_train.data, data_train.target)\n",
    "\n",
    "# Testing our model\n",
    "pipe_predict = pipe.predict(data_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Score: 0.808\n"
     ]
    }
   ],
   "source": [
    "# Accuracy Score\n",
    "print (\"Accuracy Score:\", accuracy_score(data_test.target, pipe_predict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Parameters using GridSearchCV\n",
    "Classifiers and Estimators tend to have many parameters we can get the better result when these are set perfectly. But how can we get perfect parameter for every classifier and estimator? We can tweak these parameters and check result but it's gonna take time which is very precious to us. So, to make our work easy sklearn has a method GridSearchCV(), it takes in the classifier (or estimator) and tunes it by applying different parameters.\n",
    "But remember it's a very expensive job.\n",
    "\n",
    "\n",
    "Parameters:\n",
    "> svm_clf = SGDClassifier(loss='hinge', penalty='l2', alpha=0.01, n_iter=5, random_state=42)\n",
    "\n",
    "> svm_clf.get_params\n",
    "\n",
    "Hyper-Parameters\n",
    "They are the \"higher-level\" structural information about the model, and they are typically set before training the model\n",
    "\n",
    "> pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'MNB': MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True),\n",
       " 'MNB__alpha': 1.0,\n",
       " 'MNB__class_prior': None,\n",
       " 'MNB__fit_prior': True,\n",
       " 'countvectorizer': CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "         dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "         lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "         ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "         strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "         tokenizer=None, vocabulary=None),\n",
       " 'countvectorizer__analyzer': 'word',\n",
       " 'countvectorizer__binary': False,\n",
       " 'countvectorizer__decode_error': 'strict',\n",
       " 'countvectorizer__dtype': numpy.int64,\n",
       " 'countvectorizer__encoding': 'utf-8',\n",
       " 'countvectorizer__input': 'content',\n",
       " 'countvectorizer__lowercase': True,\n",
       " 'countvectorizer__max_df': 1.0,\n",
       " 'countvectorizer__max_features': None,\n",
       " 'countvectorizer__min_df': 1,\n",
       " 'countvectorizer__ngram_range': (1, 1),\n",
       " 'countvectorizer__preprocessor': None,\n",
       " 'countvectorizer__stop_words': 'english',\n",
       " 'countvectorizer__strip_accents': None,\n",
       " 'countvectorizer__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       " 'countvectorizer__tokenizer': None,\n",
       " 'countvectorizer__vocabulary': None,\n",
       " 'steps': [('countvectorizer',\n",
       "   CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "           dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "           lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "           ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "           strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "           tokenizer=None, vocabulary=None)),\n",
       "  ('tfidf',\n",
       "   TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)),\n",
       "  ('MNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       " 'tfidf': TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True),\n",
       " 'tfidf__norm': 'l2',\n",
       " 'tfidf__smooth_idf': True,\n",
       " 'tfidf__sublinear_tf': False,\n",
       " 'tfidf__use_idf': True}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see what parameters are available for us to tune\n",
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# List of Parameters we wan to tune\n",
    "parameters = {\n",
    "    'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    #'MNB__alpha': (1.0, 0.1, 0.01, 0.001)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english...inear_tf=False, use_idf=True)), ('MNB', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))]),\n",
       "       fit_params={}, iid=True, n_jobs=-1,\n",
       "       param_grid={'MNB__alpha': (1.0, 0.1, 0.01, 0.001), 'tfidf__use_idf': (True, False), 'countvectorizer__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a Grid with the pipeline and parameter we want to tune\n",
    "gs_clf = GridSearchCV(pipe, parameters, n_jobs=-1, cv=10)\n",
    "\n",
    "# Fit the dataset\n",
    "gs_clf.fit(data_train.data, data_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Mean Score: 0.880308880309\n",
      "Best Parameter Settings:  {'MNB__alpha': 0.1, 'tfidf__use_idf': False, 'countvectorizer__ngram_range': (1, 1)}\n"
     ]
    }
   ],
   "source": [
    "# Print the best mean score and the best parameter we'll get after tuning\n",
    "print (\"Best Mean Score:\", gs_clf.best_score_)\n",
    "print (\"Best Parameter Settings: \", gs_clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuray Score: 0.768\n"
     ]
    }
   ],
   "source": [
    "# Let's use these parameters and make prediction\n",
    "gs_clf.refit\n",
    "\n",
    "# Testing model\n",
    "gs_predict = gs_clf.predict(X_test)\n",
    "\n",
    "# Accuracy Score\n",
    "print (\"Accuray Score:\", accuracy_score(data_test.target, gs_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.38943822,  1.36962538,  5.2649421 ,  5.78236973,  8.77949331,\n",
       "         8.44525688,  1.4203624 ,  1.37572818,  4.91590009,  4.83553863,\n",
       "         8.73370049,  7.88531094,  1.31288307,  1.29937325,  4.3249243 ,\n",
       "         4.20538888,  7.94481905,  7.42882466,  1.30177519,  1.28676481,\n",
       "         4.33618236,  4.19953537,  7.74887702,  7.70577683]),\n",
       " 'mean_score_time': array([ 0.13089452,  0.13259399,  0.31143126,  0.35615344,  0.39838338,\n",
       "         0.38432345,  0.14059715,  0.13324549,  0.2992125 ,  0.26744015,\n",
       "         0.38849933,  0.34439511,  0.12854149,  0.12238741,  0.25498149,\n",
       "         0.25072863,  0.36586027,  0.33889112,  0.12393808,  0.12203667,\n",
       "         0.26098549,  0.24217203,  0.36270773,  0.33814077]),\n",
       " 'mean_test_score': array([ 0.83861004,  0.83732304,  0.80733591,  0.80913771,  0.7965251 ,\n",
       "         0.8036036 ,  0.87258687,  0.88030888,  0.85791506,  0.85933076,\n",
       "         0.85070785,  0.84980695,  0.85842986,  0.87027027,  0.86718147,\n",
       "         0.87129987,  0.86718147,  0.86898327,  0.84350064,  0.85765766,\n",
       "         0.86190476,  0.86756757,  0.86241956,  0.86705277]),\n",
       " 'mean_train_score': array([ 0.9146147 ,  0.89360795,  0.93906767,  0.90444739,  0.95555565,\n",
       "         0.92009162,  0.96374953,  0.95638499,  0.97453173,  0.97275854,\n",
       "         0.97497502,  0.97454603,  0.97092816,  0.96816826,  0.97497502,\n",
       "         0.97497502,  0.97497502,  0.97497502,  0.97231525,  0.97122844,\n",
       "         0.97497502,  0.97497502,  0.97497502,  0.97497502]),\n",
       " 'param_MNB__alpha': masked_array(data = [1.0 1.0 1.0 1.0 1.0 1.0 0.1 0.1 0.1 0.1 0.1 0.1 0.01 0.01 0.01 0.01 0.01\n",
       "  0.01 0.001 0.001 0.001 0.001 0.001 0.001],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_countvectorizer__ngram_range': masked_array(data = [(1, 1) (1, 1) (1, 2) (1, 2) (1, 3) (1, 3) (1, 1) (1, 1) (1, 2) (1, 2)\n",
       "  (1, 3) (1, 3) (1, 1) (1, 1) (1, 2) (1, 2) (1, 3) (1, 3) (1, 1) (1, 1)\n",
       "  (1, 2) (1, 2) (1, 3) (1, 3)],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'param_tfidf__use_idf': masked_array(data = [True False True False True False True False True False True False True\n",
       "  False True False True False True False True False True False],\n",
       "              mask = [False False False False False False False False False False False False\n",
       "  False False False False False False False False False False False False],\n",
       "        fill_value = ?),\n",
       " 'params': ({'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 1.0,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.1,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.01,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 1),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 2),\n",
       "   'tfidf__use_idf': False},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': True},\n",
       "  {'MNB__alpha': 0.001,\n",
       "   'countvectorizer__ngram_range': (1, 3),\n",
       "   'tfidf__use_idf': False}),\n",
       " 'rank_test_score': array([19, 20, 22, 21, 24, 23,  2,  1, 14, 12, 16, 17, 13,  4,  7,  3,  7,\n",
       "         5, 18, 15, 11,  6, 10,  9]),\n",
       " 'split0_test_score': array([ 0.83697047,  0.83825417,  0.81258023,  0.81258023,  0.79974326,\n",
       "         0.80744544,  0.86392811,  0.87933248,  0.85879332,  0.85879332,\n",
       "         0.85494223,  0.85365854,  0.85879332,  0.8690629 ,  0.86264442,\n",
       "         0.86649551,  0.86649551,  0.86521181,  0.84595635,  0.85622593,\n",
       "         0.86264442,  0.86521181,  0.86264442,  0.86521181]),\n",
       " 'split0_train_score': array([ 0.91675011,  0.89386354,  0.93906451,  0.90587899,  0.95694464,\n",
       "         0.92218567,  0.96438278,  0.95622944,  0.97482477,  0.97310828,\n",
       "         0.97496782,  0.97468173,  0.97110571,  0.96924617,  0.97496782,\n",
       "         0.97496782,  0.97496782,  0.97496782,  0.97239308,  0.97139179,\n",
       "         0.97496782,  0.97496782,  0.97496782,  0.97496782]),\n",
       " 'split1_test_score': array([ 0.83161954,  0.82904884,  0.81491003,  0.81362468,  0.80462725,\n",
       "         0.80976864,  0.86246787,  0.86632391,  0.85089974,  0.85604113,\n",
       "         0.84575835,  0.84447301,  0.84575835,  0.85218509,  0.8496144 ,\n",
       "         0.85218509,  0.85218509,  0.85347044,  0.82647815,  0.85218509,\n",
       "         0.8470437 ,  0.85347044,  0.84832905,  0.85218509]),\n",
       " 'split1_train_score': array([ 0.91475973,  0.89402174,  0.94050343,  0.90431922,  0.95709382,\n",
       "         0.92048055,  0.96438787,  0.95637872,  0.97482838,  0.97339817,\n",
       "         0.97525744,  0.97482838,  0.97168192,  0.96882151,  0.97525744,\n",
       "         0.97525744,  0.97525744,  0.97525744,  0.97311213,  0.97196796,\n",
       "         0.97525744,  0.97525744,  0.97525744,  0.97525744]),\n",
       " 'split2_test_score': array([ 0.83676093,  0.8277635 ,  0.80205656,  0.81105398,  0.79177378,\n",
       "         0.80462725,  0.88174807,  0.88946015,  0.85604113,  0.85732648,\n",
       "         0.85089974,  0.8470437 ,  0.86503856,  0.87403599,  0.87789203,\n",
       "         0.87789203,  0.87532134,  0.87532134,  0.84318766,  0.85604113,\n",
       "         0.8714653 ,  0.87403599,  0.86760925,  0.87660668]),\n",
       " 'split2_train_score': array([ 0.91404462,  0.89402174,  0.93778604,  0.90446224,  0.95466247,\n",
       "         0.91905034,  0.9625286 ,  0.95537757,  0.97525744,  0.97296911,\n",
       "         0.97582952,  0.97525744,  0.97125286,  0.96824943,  0.97582952,\n",
       "         0.97582952,  0.97582952,  0.97582952,  0.97268307,  0.97139588,\n",
       "         0.97582952,  0.97582952,  0.97582952,  0.97582952]),\n",
       " 'split3_test_score': array([ 0.8277635 ,  0.83290488,  0.79948586,  0.79820051,  0.7840617 ,\n",
       "         0.79177378,  0.85861183,  0.86760925,  0.84061697,  0.84061697,\n",
       "         0.82647815,  0.83161954,  0.84318766,  0.86118252,  0.85604113,\n",
       "         0.85989717,  0.85604113,  0.85861183,  0.81876607,  0.83933162,\n",
       "         0.85218509,  0.85732648,  0.85347044,  0.85475578]),\n",
       " 'split3_train_score': array([ 0.91404462,  0.89273455,  0.93935927,  0.90360412,  0.95680778,\n",
       "         0.91947941,  0.96495995,  0.95780892,  0.97454233,  0.97339817,\n",
       "         0.97511442,  0.97468535,  0.9715389 ,  0.96853547,  0.97511442,\n",
       "         0.97511442,  0.97511442,  0.97511442,  0.97325515,  0.97168192,\n",
       "         0.97511442,  0.97511442,  0.97511442,  0.97511442]),\n",
       " 'split4_test_score': array([ 0.83526384,  0.83140283,  0.8018018 ,  0.8018018 ,  0.79407979,\n",
       "         0.7992278 ,  0.85842986,  0.87001287,  0.85070785,  0.85328185,\n",
       "         0.84041184,  0.83912484,  0.84684685,  0.85971686,  0.85714286,\n",
       "         0.86229086,  0.85456885,  0.85585586,  0.83397683,  0.84684685,\n",
       "         0.84555985,  0.85585586,  0.84684685,  0.85328185]),\n",
       " 'split4_train_score': array([ 0.91577292,  0.8951809 ,  0.93965394,  0.90561991,  0.95509796,\n",
       "         0.92092092,  0.96296296,  0.95609896,  0.97425997,  0.97182897,\n",
       "         0.97454597,  0.97425997,  0.97039897,  0.96768197,  0.97454597,\n",
       "         0.97454597,  0.97454597,  0.97454597,  0.97225797,  0.97111397,\n",
       "         0.97454597,  0.97454597,  0.97454597,  0.97454597]),\n",
       " 'split5_test_score': array([ 0.83526384,  0.83397683,  0.79407979,  0.7979408 ,  0.77992278,\n",
       "         0.78764479,  0.85971686,  0.87129987,  0.83783784,  0.84427284,\n",
       "         0.82882883,  0.82882883,  0.84169884,  0.85971686,  0.83912484,\n",
       "         0.85971686,  0.84041184,  0.85070785,  0.84298584,  0.84684685,\n",
       "         0.83912484,  0.85070785,  0.83526384,  0.84298584]),\n",
       " 'split5_train_score': array([ 0.91448591,  0.89417989,  0.93951094,  0.9044759 ,  0.95509796,\n",
       "         0.92034892,  0.96453596,  0.95695696,  0.97554698,  0.97354497,\n",
       "         0.97597598,  0.97554698,  0.97197197,  0.96896897,  0.97597598,\n",
       "         0.97597598,  0.97597598,  0.97597598,  0.97311597,  0.97197197,\n",
       "         0.97597598,  0.97597598,  0.97597598,  0.97597598]),\n",
       " 'split6_test_score': array([ 0.83783784,  0.84298584,  0.8018018 ,  0.7979408 ,  0.79021879,\n",
       "         0.79279279,  0.88030888,  0.88416988,  0.86872587,  0.86100386,\n",
       "         0.86100386,  0.85585586,  0.86743887,  0.88416988,  0.87001287,\n",
       "         0.87001287,  0.87258687,  0.87258687,  0.85070785,  0.87001287,\n",
       "         0.86615187,  0.87258687,  0.87001287,  0.87516088]),\n",
       " 'split6_train_score': array([ 0.91405691,  0.89317889,  0.93936794,  0.90533391,  0.95495495,\n",
       "         0.92092092,  0.96424996,  0.95709996,  0.97440297,  0.97311597,\n",
       "         0.97483197,  0.97454597,  0.97097097,  0.96796797,  0.97483197,\n",
       "         0.97483197,  0.97483197,  0.97483197,  0.97182897,  0.97111397,\n",
       "         0.97483197,  0.97483197,  0.97483197,  0.97483197]),\n",
       " 'split7_test_score': array([ 0.8492268 ,  0.84407216,  0.81701031,  0.81314433,  0.81056701,\n",
       "         0.81443299,  0.89175258,  0.88917526,  0.875     ,  0.87242268,\n",
       "         0.8621134 ,  0.86469072,  0.88402062,  0.88659794,  0.89175258,\n",
       "         0.88917526,  0.88917526,  0.88659794,  0.86469072,  0.88015464,\n",
       "         0.88015464,  0.88917526,  0.88659794,  0.88917526]),\n",
       " 'split7_train_score': array([ 0.91378324,  0.89305119,  0.93723191,  0.90334572,  0.95381756,\n",
       "         0.91864455,  0.96311124,  0.95596225,  0.97297684,  0.9711181 ,\n",
       "         0.97369174,  0.97297684,  0.96997426,  0.96697169,  0.97369174,\n",
       "         0.97369174,  0.97369174,  0.97369174,  0.97054618,  0.96997426,\n",
       "         0.97369174,  0.97369174,  0.97369174,  0.97369174]),\n",
       " 'split8_test_score': array([ 0.84903226,  0.84645161,  0.81419355,  0.82322581,  0.80387097,\n",
       "         0.81032258,  0.88387097,  0.89548387,  0.86193548,  0.86967742,\n",
       "         0.86064516,  0.85548387,  0.86193548,  0.87612903,  0.87870968,\n",
       "         0.88516129,  0.87741935,  0.88258065,  0.85032258,  0.85677419,\n",
       "         0.8683871 ,  0.87354839,  0.8683871 ,  0.87354839]),\n",
       " 'split8_train_score': array([ 0.91379557,  0.89292352,  0.93967119,  0.90293066,  0.95596855,\n",
       "         0.91951394,  0.96297355,  0.95553967,  0.97498213,  0.97340958,\n",
       "         0.97541101,  0.97498213,  0.97069335,  0.96769121,  0.97541101,\n",
       "         0.97541101,  0.97541101,  0.97541101,  0.97255182,  0.97155111,\n",
       "         0.97541101,  0.97541101,  0.97541101,  0.97541101]),\n",
       " 'split9_test_score': array([ 0.84645161,  0.84645161,  0.81548387,  0.82193548,  0.80645161,\n",
       "         0.81806452,  0.88516129,  0.89032258,  0.87870968,  0.88      ,\n",
       "         0.87612903,  0.87741935,  0.86967742,  0.88      ,  0.88903226,\n",
       "         0.89032258,  0.88774194,  0.88903226,  0.85806452,  0.87225806,\n",
       "         0.88645161,  0.88387097,  0.88516129,  0.88774194]),\n",
       " 'split9_train_score': array([ 0.91465332,  0.89292352,  0.93852752,  0.90450322,  0.95511079,\n",
       "         0.91937098,  0.96340243,  0.95639743,  0.9736955 ,  0.97169407,\n",
       "         0.97412437,  0.9736955 ,  0.96969264,  0.96754825,  0.97412437,\n",
       "         0.97412437,  0.97412437,  0.97412437,  0.97140815,  0.97012152,\n",
       "         0.97412437,  0.97412437,  0.97412437,  0.97412437]),\n",
       " 'std_fit_time': array([ 0.01542764,  0.02164887,  0.27022228,  0.97560502,  0.25667911,\n",
       "         0.13580669,  0.04428127,  0.03608491,  0.11748181,  0.16062464,\n",
       "         0.53089066,  0.78286849,  0.0296787 ,  0.03338283,  0.03523376,\n",
       "         0.02739911,  0.1769672 ,  0.04851246,  0.0150009 ,  0.01028111,\n",
       "         0.05387046,  0.06526209,  0.1069692 ,  0.23625865]),\n",
       " 'std_score_time': array([ 0.00452758,  0.00928036,  0.031722  ,  0.09842925,  0.04715113,\n",
       "         0.0170036 ,  0.01451507,  0.00577863,  0.02012685,  0.01673281,\n",
       "         0.03264274,  0.02175957,  0.00854826,  0.00103024,  0.00864364,\n",
       "         0.01075107,  0.02573932,  0.01124388,  0.00146823,  0.00110657,\n",
       "         0.01300186,  0.00449199,  0.01487931,  0.02636762]),\n",
       " 'std_test_score': array([ 0.00691436,  0.00685934,  0.00786257,  0.009151  ,  0.00961595,\n",
       "         0.0097748 ,  0.01238763,  0.01025876,  0.01289974,  0.01155858,\n",
       "         0.0148124 ,  0.0140956 ,  0.01313393,  0.01112073,  0.01635995,\n",
       "         0.01285055,  0.01526586,  0.0134821 ,  0.01325972,  0.01214989,\n",
       "         0.01479005,  0.0125106 ,  0.01574428,  0.01503307]),\n",
       " 'std_train_score': array([ 0.00090906,  0.00073504,  0.00091899,  0.00091956,  0.00104067,\n",
       "         0.00101902,  0.00079887,  0.00069859,  0.00071705,  0.00082724,\n",
       "         0.00067797,  0.00071297,  0.00070293,  0.00068495,  0.00067797,\n",
       "         0.00067797,  0.00067797,  0.00067797,  0.00080926,  0.00065488,\n",
       "         0.00067797,  0.00067797,  0.00067797,  0.00067797])}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_clf.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Pipeline_RidgeClassifier_InShortsNews.sav']"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(pipe, 'Pipeline_RidgeClassifier_InShortsNews.sav')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use these models in future for predicting data without needing to train it. Model is Pre-trained\n",
    "\n",
    "To load the model\n",
    "> model = joblib.load(model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
